{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd32179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage.util import random_noise\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401657de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, sequence_length=5, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.frames = self._load_frames_from_directory(video_dir)\n",
    "\n",
    "    def _load_frames_from_directory(self, video_dir):\n",
    "        # Charger les frames depuis un r√©pertoire\n",
    "        files = sorted([f for f in os.listdir(video_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        frames = []\n",
    "        for file in files:\n",
    "            frame_path = os.path.join(video_dir, file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is not None:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "        return frames\n",
    "\n",
    "    def add_gaussian_noise(self, frame, mean=0, std=0.1):\n",
    "        noise = np.random.normal(mean, std, frame.shape)\n",
    "        noisy_frame = frame + noise\n",
    "        return np.clip(noisy_frame, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def add_salt_pepper_noise(self, frame, amount=0.02):\n",
    "        noisy_frame = random_noise(frame / 255.0, mode='s&p', amount=amount)\n",
    "        return (noisy_frame * 255).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.frames) - self.sequence_length + 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.frames[idx:idx + self.sequence_length]\n",
    "        noisy_frames = [self.add_gaussian_noise(f) for f in frames]\n",
    "        noisy_frames = [self.add_salt_pepper_noise(f) for f in noisy_frames]\n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(Image.fromarray(f)) for f in frames])\n",
    "            noisy_frames = torch.stack([self.transform(Image.fromarray(f)) for f in noisy_frames])\n",
    "        return noisy_frames, frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f721d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGuidedBlock(nn.Module):\n",
    "    def __init__(self, nIn, nOut, dilation_rate=2):\n",
    "        super(ContextGuidedBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(nIn, nOut // 2, kernel_size=1)\n",
    "        self.conv3x3 = nn.Conv2d(nOut // 2, nOut, kernel_size=3, padding=dilation_rate, dilation=dilation_rate)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    def __init__(self, nIn, nOut):\n",
    "        super(ContextModule, self).__init__()\n",
    "        self.conv_dil1 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_dil2 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_dil3 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_dil1(x)\n",
    "        x2 = self.conv_dil2(x)\n",
    "        x3 = self.conv_dil3(x)\n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class CGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGNet, self).__init__()\n",
    "        self.block1 = ContextGuidedBlock(3, 32)\n",
    "        self.block2 = ContextGuidedBlock(32, 64)\n",
    "        self.res_block1 = self._make_res_block(64, 64)\n",
    "        self.context_module = ContextModule(64, 64)\n",
    "        self.res_block2 = self._make_res_block(64, 64)\n",
    "        self.conv_final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "    \n",
    "    def _make_res_block(self, nIn, nOut):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(nIn, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(nOut, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "        x2 = self.res_block1(x2)\n",
    "        x_context = self.context_module(x2)\n",
    "        x3 = self.res_block2(x_context)\n",
    "        output = self.conv_final(x3)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset charg√© avec 74 s√©quences vid√©o.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Initialisation du mod√®le CGNet pour la vid√©o\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CGNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # Fonction de perte\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimiseur\n",
    "\n",
    "# Charger les donn√©es d'entra√Ænement\n",
    "sequence_length = 5\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "video_dir = r\"DAVIS/DAVIS/JPEGImages/Full-Resolution/gold-fish\"\n",
    "dataset = VideoDataset(video_dir, sequence_length, transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(f\"Dataset charg√© avec {len(dataset)} s√©quences vid√©o.\")\n",
    "\n",
    "# Entra√Ænement\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_frames, clean_frames in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # noisy_frames: [batch_size, sequence_length, channels, height, width]\n",
    "        # clean_frames: [batch_size, sequence_length, channels, height, width]\n",
    "        \n",
    "        noisy_frames = noisy_frames.squeeze(0)  # Supprimer batch_size (1) temporairement\n",
    "        clean_frames = clean_frames.squeeze(0)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for t in range(sequence_length):\n",
    "            noisy_frame = noisy_frames[t].unsqueeze(0).to(device)  # Ajouter batch_size (1)\n",
    "            clean_frame = clean_frames[t].unsqueeze(0).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(noisy_frame)  # Passer une frame √† la fois\n",
    "            loss = criterion(outputs, clean_frame)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "        \n",
    "        running_loss += batch_loss / sequence_length  # Moyenne des pertes pour la s√©quence\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(loader):.4f}\")\n",
    "\n",
    "# Sauvegarder le mod√®le entra√Æn√©\n",
    "model_path = \"cgnet_video.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Mod√®le entra√Æn√© et sauvegard√© sous : {model_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cf0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_video(video_path, output_path, model, device, transform):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Le fichier vid√©o {video_path} est introuvable.\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Configuration pour la vid√©o de sortie\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec MP4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(\"D√©bruitage de la vid√©o...\")\n",
    "    frame_count = 0  # Compteur de frames trait√©es\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Pr√©traitement de la frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "        input_tensor = transform(frame_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # D√©bruitage avec le mod√®le\n",
    "        with torch.no_grad():\n",
    "            output_tensor = model(input_tensor).squeeze(0).cpu()\n",
    "        \n",
    "        # Post-traitement\n",
    "        output_frame = (output_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "        output_frame_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # √âcriture de la frame dans la vid√©o de sortie\n",
    "        out.write(output_frame_bgr)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Vid√©o d√©bruit√©e sauvegard√©e dans : {output_path}\")\n",
    "    print(f\"Nombre de frames trait√©es : {frame_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912992b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©bruitage de la vid√©o...\n",
      "Vid√©o d√©bruit√©e sauvegard√©e dans : video_denoised/surf_denoised.mp4\n",
      "Nombre de frames trait√©es : 255\n"
     ]
    }
   ],
   "source": [
    "# D√©bruiter une vid√©o\n",
    "input_video = \"video_noised/gold-fish_noised.mp4\"\n",
    "output_video = \"video_denoised/gold-fish_denoised.mp4\"\n",
    "denoise_video(input_video, output_video, model, device, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(image_tensor.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        return torch.sum(scores * torch.arange(1, 11).to(device)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_with_metrics(model, dataset, index):\n",
    "    \"\"\"\n",
    "    Afficher les images bruit√©es, originales, et d√©bruit√©es avec les scores PSNR et NIMA.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    noisy_img, clean_img = dataset[index]\n",
    "\n",
    "    noisy_img = noisy_img.unsqueeze(0).to(device)\n",
    "    clean_img = clean_img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(noisy_img)\n",
    "        output = output.cpu().squeeze()\n",
    "    \n",
    "    psnr_noisy = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), noisy_img.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    psnr_denoised = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), output.permute(1, 2, 0).numpy())\n",
    "    \n",
    "    nima_noisy = calculate_nima_score(noisy_img.squeeze())\n",
    "    nima_clean = calculate_nima_score(clean_img.squeeze())\n",
    "    nima_denoised = calculate_nima_score(output)\n",
    "    \n",
    "    print(f\"PSNR Noisy: {psnr_noisy:.2f}, PSNR Denoised: {psnr_denoised:.2f}\")\n",
    "    print(f\"NIMA Noisy: {nima_noisy:.2f}, NIMA Clean: {nima_clean:.2f}, NIMA Denoised: {nima_denoised:.2f}\")\n",
    "    \n",
    "    # Pr√©parer les images pour affichage\n",
    "    noisy_img = noisy_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    clean_img = clean_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    output = output.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # S'assurer que les valeurs des images sont entre 0 et 1\n",
    "    noisy_img = np.clip(noisy_img, 0, 1)\n",
    "    clean_img = np.clip(clean_img, 0, 1)\n",
    "    output = np.clip(output, 0, 1)\n",
    "    \n",
    "    # Afficher les images\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Noisy Image\\nNIMA: {nima_noisy:.2f}\")\n",
    "    plt.imshow(noisy_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Clean Image\\nNIMA: {nima_clean:.2f}\")\n",
    "    plt.imshow(clean_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}, NIMA: {nima_denoised:.2f}\")\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Forcer l'installation de nbformat dans l'environnement actuel du notebook\n",
    "!{sys.executable} -m pip install --upgrade nbformat\n",
    "\n",
    "import nbformat\n",
    "import zipfile\n",
    "\n",
    "def clear_notebook_output(notebook_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Nettoyer toutes les sorties d'un notebook Jupyter pour le rendre plus l√©ger.\n",
    "    \"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "\n",
    "    output_path = output_path or notebook_path\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    print(f\"Nettoyage termin√© ! Notebook all√©g√© sauvegard√© sous : {output_path}\")\n",
    "\n",
    "def zip_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Compresser le notebook nettoy√© pour r√©duire sa taille.\n",
    "    \"\"\"\n",
    "    zip_path = notebook_path.replace(\".ipynb\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(notebook_path)\n",
    "    print(f\"Notebook compress√© et sauvegard√© sous : {zip_path}\")\n",
    "\n",
    "def delete_large_files():\n",
    "    \"\"\"\n",
    "    Supprimer tous les fichiers volumineux tels que .pth, .h5 et les checkpoints.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\") or file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Fichier supprim√© : {file_path}\")\n",
    "\n",
    "\n",
    "    if os.path.exists('.ipynb_checkpoints'):\n",
    "        os.system(\"rm -rf .ipynb_checkpoints\")\n",
    "        print(\"Checkpoints supprim√©s.\")\n",
    "\n",
    "    print(\"Nettoyage des fichiers volumineux termin√© !\")\n",
    "\n",
    "notebook_name = \"CGNet_BSD500.ipynb\"\n",
    "\n",
    "\n",
    "clear_notebook_output(notebook_name, \"notebook_all√©g√©.ipynb\")\n",
    "zip_notebook(\"notebook_all√©g√©.ipynb\")\n",
    "delete_large_files()\n",
    "\n",
    "print(\"\\nüì¶ Op√©rations termin√©es ! Le notebook a √©t√© nettoy√©, compress√© et les fichiers inutiles supprim√©s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54360735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
