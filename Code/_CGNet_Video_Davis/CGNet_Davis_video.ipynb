{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd32179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage.util import random_noise\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401657de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir, sequence_length=5, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.frames = self._load_frames_from_directory(video_dir)\n",
    "\n",
    "    def _load_frames_from_directory(self, video_dir):\n",
    "        # Charger les frames depuis un répertoire\n",
    "        files = sorted([f for f in os.listdir(video_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        frames = []\n",
    "        for file in files:\n",
    "            frame_path = os.path.join(video_dir, file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is not None:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "        return frames\n",
    "\n",
    "    def add_gaussian_noise(self, frame, mean=0, std=0.1):\n",
    "        noise = np.random.normal(mean, std, frame.shape)\n",
    "        noisy_frame = frame + noise\n",
    "        return np.clip(noisy_frame, 0, 255).astype(np.uint8)\n",
    "\n",
    "    def add_salt_pepper_noise(self, frame, amount=0.02):\n",
    "        noisy_frame = random_noise(frame / 255.0, mode='s&p', amount=amount)\n",
    "        return (noisy_frame * 255).astype(np.uint8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.frames) - self.sequence_length + 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.frames[idx:idx + self.sequence_length]\n",
    "        noisy_frames = [self.add_gaussian_noise(f) for f in frames]\n",
    "        noisy_frames = [self.add_salt_pepper_noise(f) for f in noisy_frames]\n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(Image.fromarray(f)) for f in frames])\n",
    "            noisy_frames = torch.stack([self.transform(Image.fromarray(f)) for f in noisy_frames])\n",
    "        return noisy_frames, frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f721d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGuidedBlock(nn.Module):\n",
    "    def __init__(self, nIn, nOut, dilation_rate=2):\n",
    "        super(ContextGuidedBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(nIn, nOut // 2, kernel_size=1)\n",
    "        self.conv3x3 = nn.Conv2d(nOut // 2, nOut, kernel_size=3, padding=dilation_rate, dilation=dilation_rate)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    def __init__(self, nIn, nOut):\n",
    "        super(ContextModule, self).__init__()\n",
    "        self.conv_dil1 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_dil2 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_dil3 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_dil1(x)\n",
    "        x2 = self.conv_dil2(x)\n",
    "        x3 = self.conv_dil3(x)\n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class CGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGNet, self).__init__()\n",
    "        self.block1 = ContextGuidedBlock(3, 32)\n",
    "        self.block2 = ContextGuidedBlock(32, 64)\n",
    "        self.res_block1 = self._make_res_block(64, 64)\n",
    "        self.context_module = ContextModule(64, 64)\n",
    "        self.res_block2 = self._make_res_block(64, 64)\n",
    "        self.conv_final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "    \n",
    "    def _make_res_block(self, nIn, nOut):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(nIn, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(nOut, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "        x2 = self.res_block1(x2)\n",
    "        x_context = self.context_module(x2)\n",
    "        x3 = self.res_block2(x_context)\n",
    "        output = self.conv_final(x3)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé avec 74 séquences vidéo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/74 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Initialisation du modèle CGNet pour la vidéo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CGNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  # Fonction de perte\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimiseur\n",
    "\n",
    "# Charger les données d'entraînement\n",
    "sequence_length = 5\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "video_dir = r\"DAVIS/DAVIS/JPEGImages/Full-Resolution/gold-fish\"\n",
    "dataset = VideoDataset(video_dir, sequence_length, transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "print(f\"Dataset chargé avec {len(dataset)} séquences vidéo.\")\n",
    "\n",
    "# Entraînement\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_frames, clean_frames in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # noisy_frames: [batch_size, sequence_length, channels, height, width]\n",
    "        # clean_frames: [batch_size, sequence_length, channels, height, width]\n",
    "        \n",
    "        noisy_frames = noisy_frames.squeeze(0)  # Supprimer batch_size (1) temporairement\n",
    "        clean_frames = clean_frames.squeeze(0)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for t in range(sequence_length):\n",
    "            noisy_frame = noisy_frames[t].unsqueeze(0).to(device)  # Ajouter batch_size (1)\n",
    "            clean_frame = clean_frames[t].unsqueeze(0).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(noisy_frame)  # Passer une frame à la fois\n",
    "            loss = criterion(outputs, clean_frame)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "        \n",
    "        running_loss += batch_loss / sequence_length  # Moyenne des pertes pour la séquence\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(loader):.4f}\")\n",
    "\n",
    "# Sauvegarder le modèle entraîné\n",
    "model_path = \"cgnet_video.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Modèle entraîné et sauvegardé sous : {model_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8cf0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_video(video_path, output_path, model, device, transform):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Le fichier vidéo {video_path} est introuvable.\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Configuration pour la vidéo de sortie\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec MP4\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(\"Débruitage de la vidéo...\")\n",
    "    frame_count = 0  # Compteur de frames traitées\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Prétraitement de la frame\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "        input_tensor = transform(frame_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Débruitage avec le modèle\n",
    "        with torch.no_grad():\n",
    "            output_tensor = model(input_tensor).squeeze(0).cpu()\n",
    "        \n",
    "        # Post-traitement\n",
    "        output_frame = (output_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "        output_frame_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Écriture de la frame dans la vidéo de sortie\n",
    "        out.write(output_frame_bgr)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Vidéo débruitée sauvegardée dans : {output_path}\")\n",
    "    print(f\"Nombre de frames traitées : {frame_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912992b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Débruitage de la vidéo...\n",
      "Vidéo débruitée sauvegardée dans : video_denoised/surf_denoised.mp4\n",
      "Nombre de frames traitées : 255\n"
     ]
    }
   ],
   "source": [
    "# Débruiter une vidéo\n",
    "input_video = \"video_noised/gold-fish_noised.mp4\"\n",
    "output_video = \"video_denoised/gold-fish_denoised.mp4\"\n",
    "denoise_video(input_video, output_video, model, device, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(image_tensor.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        return torch.sum(scores * torch.arange(1, 11).to(device)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_with_metrics(model, dataset, index):\n",
    "    \"\"\"\n",
    "    Afficher les images bruitées, originales, et débruitées avec les scores PSNR et NIMA.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    noisy_img, clean_img = dataset[index]\n",
    "\n",
    "    noisy_img = noisy_img.unsqueeze(0).to(device)\n",
    "    clean_img = clean_img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(noisy_img)\n",
    "        output = output.cpu().squeeze()\n",
    "    \n",
    "    psnr_noisy = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), noisy_img.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    psnr_denoised = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), output.permute(1, 2, 0).numpy())\n",
    "    \n",
    "    nima_noisy = calculate_nima_score(noisy_img.squeeze())\n",
    "    nima_clean = calculate_nima_score(clean_img.squeeze())\n",
    "    nima_denoised = calculate_nima_score(output)\n",
    "    \n",
    "    print(f\"PSNR Noisy: {psnr_noisy:.2f}, PSNR Denoised: {psnr_denoised:.2f}\")\n",
    "    print(f\"NIMA Noisy: {nima_noisy:.2f}, NIMA Clean: {nima_clean:.2f}, NIMA Denoised: {nima_denoised:.2f}\")\n",
    "    \n",
    "    # Préparer les images pour affichage\n",
    "    noisy_img = noisy_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    clean_img = clean_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    output = output.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # S'assurer que les valeurs des images sont entre 0 et 1\n",
    "    noisy_img = np.clip(noisy_img, 0, 1)\n",
    "    clean_img = np.clip(clean_img, 0, 1)\n",
    "    output = np.clip(output, 0, 1)\n",
    "    \n",
    "    # Afficher les images\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Noisy Image\\nNIMA: {nima_noisy:.2f}\")\n",
    "    plt.imshow(noisy_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Clean Image\\nNIMA: {nima_clean:.2f}\")\n",
    "    plt.imshow(clean_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}, NIMA: {nima_denoised:.2f}\")\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Forcer l'installation de nbformat dans l'environnement actuel du notebook\n",
    "!{sys.executable} -m pip install --upgrade nbformat\n",
    "\n",
    "import nbformat\n",
    "import zipfile\n",
    "\n",
    "def clear_notebook_output(notebook_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Nettoyer toutes les sorties d'un notebook Jupyter pour le rendre plus léger.\n",
    "    \"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "\n",
    "    output_path = output_path or notebook_path\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    print(f\"Nettoyage terminé ! Notebook allégé sauvegardé sous : {output_path}\")\n",
    "\n",
    "def zip_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Compresser le notebook nettoyé pour réduire sa taille.\n",
    "    \"\"\"\n",
    "    zip_path = notebook_path.replace(\".ipynb\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(notebook_path)\n",
    "    print(f\"Notebook compressé et sauvegardé sous : {zip_path}\")\n",
    "\n",
    "def delete_large_files():\n",
    "    \"\"\"\n",
    "    Supprimer tous les fichiers volumineux tels que .pth, .h5 et les checkpoints.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\") or file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Fichier supprimé : {file_path}\")\n",
    "\n",
    "\n",
    "    if os.path.exists('.ipynb_checkpoints'):\n",
    "        os.system(\"rm -rf .ipynb_checkpoints\")\n",
    "        print(\"Checkpoints supprimés.\")\n",
    "\n",
    "    print(\"Nettoyage des fichiers volumineux terminé !\")\n",
    "\n",
    "notebook_name = \"CGNet_BSD500.ipynb\"\n",
    "\n",
    "\n",
    "clear_notebook_output(notebook_name, \"notebook_allégé.ipynb\")\n",
    "zip_notebook(\"notebook_allégé.ipynb\")\n",
    "delete_large_files()\n",
    "\n",
    "print(\"\\n📦 Opérations terminées ! Le notebook a été nettoyé, compressé et les fichiers inutiles supprimés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54360735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
