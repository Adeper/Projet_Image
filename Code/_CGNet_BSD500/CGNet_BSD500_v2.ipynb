{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd32179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08415a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) \n",
    "                            if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def add_gaussian_noise(self, image):\n",
    "        noise_level = np.random.uniform(0.02, 0.1)\n",
    "        noisy_image = image + noise_level * torch.randn_like(image)\n",
    "        return torch.clip(noisy_image, 0., 1.)\n",
    "\n",
    "    def add_salt_pepper_noise(self, image, prob=0.02):\n",
    "        noisy_image = image.clone()\n",
    "        num_pixels = image.numel()\n",
    "        num_pepper = int(prob * num_pixels * 0.5)\n",
    "        pepper_coords = (torch.randint(0, image.shape[1], (num_pepper,)), torch.randint(0, image.shape[2], (num_pepper,)))\n",
    "        noisy_image[:, pepper_coords[0], pepper_coords[1]] = 0\n",
    "        num_salt = int(prob * num_pixels * 0.5)\n",
    "        salt_coords = (torch.randint(0, image.shape[1], (num_salt,)), torch.randint(0, image.shape[2], (num_salt,)))\n",
    "        noisy_image[:, salt_coords[0], salt_coords[1]] = 1\n",
    "        return noisy_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        noisy_image = self.add_gaussian_noise(image)\n",
    "        noisy_image = self.add_salt_pepper_noise(noisy_image)\n",
    "        return noisy_image, image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = DenoisingDataset(root_dir='archive/images/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f721d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGuidedBlock(nn.Module):\n",
    "    def __init__(self, nIn, nOut, dilation_rate=2):\n",
    "        super(ContextGuidedBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(nIn, nOut // 2, kernel_size=1)\n",
    "        self.conv3x3 = nn.Conv2d(nOut // 2, nOut, kernel_size=3, padding=dilation_rate, dilation=dilation_rate)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    def __init__(self, nIn, nOut):\n",
    "        super(ContextModule, self).__init__()\n",
    "        self.conv_dil1 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_dil2 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_dil3 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_dil1(x)\n",
    "        x2 = self.conv_dil2(x)\n",
    "        x3 = self.conv_dil3(x)\n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class CGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGNet, self).__init__()\n",
    "        self.block1 = ContextGuidedBlock(3, 32)\n",
    "        self.block2 = ContextGuidedBlock(32, 64)\n",
    "        self.res_block1 = self._make_res_block(64, 64)\n",
    "        self.context_module = ContextModule(64, 64)\n",
    "        self.res_block2 = self._make_res_block(64, 64)\n",
    "        self.conv_final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "    \n",
    "    def _make_res_block(self, nIn, nOut):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(nIn, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(nOut, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "        x2 = self.res_block1(x2)\n",
    "        x_context = self.context_module(x2)\n",
    "        x3 = self.res_block2(x_context)\n",
    "        output = self.conv_final(x3)\n",
    "        return output\n",
    "\n",
    "model = CGNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d28436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "for file in glob.glob(\"archive/images/train/Thumbs.db\"):\n",
    "    os.remove(file)\n",
    "print(\"Tous les fichiers Thumbs.db ont √©t√© supprim√©s.\")\n",
    "\n",
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    \"\"\"\n",
    "    Calcule le score NIMA apr√®s un pr√©-traitement pour am√©liorer la qualit√© per√ßue.\n",
    "    \"\"\"\n",
    "    enhanced_image = enhance_image(image_tensor)\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(enhanced_image.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        nima_score = torch.sum(scores * torch.arange(1, 11).to(device)).item()\n",
    "    return nima_score\n",
    "def enhance_image(image):\n",
    "    \"\"\"\n",
    "    Pr√©-traitement pour am√©liorer la qualit√© esth√©tique avant d'utiliser NIMA.\n",
    "    \"\"\"\n",
    "    image = F.adjust_saturation(image, 1.8)\n",
    "    image = F.adjust_contrast(image, 1.5)\n",
    "    image = F.adjust_brightness(image, 1.2)\n",
    "    \n",
    "    sharpen_kernel = torch.tensor(\n",
    "        [[[[0, -1, 0], [-1, 5, -1], [0, -1, 0]]]], \n",
    "        dtype=torch.float32, \n",
    "        device=device\n",
    "    )\n",
    "    channels = []\n",
    "    for c in range(3):\n",
    "        channel = image[c:c+1].unsqueeze(0)\n",
    "        sharpened_channel = torch.nn.functional.conv2d(channel, sharpen_kernel, padding=1)\n",
    "        channels.append(sharpened_channel)\n",
    "    enhanced_image = torch.cat(channels, dim=1)\n",
    "    return torch.clip(enhanced_image, 0., 1.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Entra√Ænement du mod√®le CGNet avec supervision multi-niveaux\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_imgs, clean_imgs in tqdm(train_loader):\n",
    "        noisy_imgs = noisy_imgs.to(device)\n",
    "        clean_imgs = clean_imgs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, intermediate = model(noisy_imgs)\n",
    "        \n",
    "        loss1 = criterion(outputs, clean_imgs)  \n",
    "        loss2 = criterion(intermediate, clean_imgs)  \n",
    "        loss = loss1 + 0.5 * loss2  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'cgnet_complete.pth')\n",
    "print(\"Mod√®le sauvegard√©.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(image_tensor.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        return torch.sum(scores * torch.arange(1, 11).to(device)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_with_metrics(model, dataset, index):\n",
    "    \"\"\"\n",
    "    Afficher les images bruit√©es, originales, et d√©bruit√©es avec les scores PSNR et NIMA.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    noisy_img, clean_img = dataset[index]\n",
    "\n",
    "    noisy_img = noisy_img.unsqueeze(0).to(device)\n",
    "    clean_img = clean_img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(noisy_img)\n",
    "        output = output.cpu().squeeze()\n",
    "    \n",
    "    psnr_noisy = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), noisy_img.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    psnr_denoised = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), output.permute(1, 2, 0).numpy())\n",
    "    \n",
    "    nima_noisy = calculate_nima_score(noisy_img.squeeze())\n",
    "    nima_clean = calculate_nima_score(clean_img.squeeze())\n",
    "    nima_denoised = calculate_nima_score(output)\n",
    "    \n",
    "    print(f\"PSNR Noisy: {psnr_noisy:.2f}, PSNR Denoised: {psnr_denoised:.2f}\")\n",
    "    print(f\"NIMA Noisy: {nima_noisy:.2f}, NIMA Clean: {nima_clean:.2f}, NIMA Denoised: {nima_denoised:.2f}\")\n",
    "    \n",
    "    # Pr√©parer les images pour affichage\n",
    "    noisy_img = noisy_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    clean_img = clean_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    output = output.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # S'assurer que les valeurs des images sont entre 0 et 1\n",
    "    noisy_img = np.clip(noisy_img, 0, 1)\n",
    "    clean_img = np.clip(clean_img, 0, 1)\n",
    "    output = np.clip(output, 0, 1)\n",
    "    \n",
    "    # Afficher les images\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Noisy Image\\nNIMA: {nima_noisy:.2f}\")\n",
    "    plt.imshow(noisy_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Clean Image\\nNIMA: {nima_clean:.2f}\")\n",
    "    plt.imshow(clean_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}, NIMA: {nima_denoised:.2f}\")\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Forcer l'installation de nbformat dans l'environnement actuel du notebook\n",
    "!{sys.executable} -m pip install --upgrade nbformat\n",
    "\n",
    "import nbformat\n",
    "import zipfile\n",
    "\n",
    "def clear_notebook_output(notebook_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Nettoyer toutes les sorties d'un notebook Jupyter pour le rendre plus l√©ger.\n",
    "    \"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "\n",
    "    output_path = output_path or notebook_path\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    print(f\"Nettoyage termin√© ! Notebook all√©g√© sauvegard√© sous : {output_path}\")\n",
    "\n",
    "def zip_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Compresser le notebook nettoy√© pour r√©duire sa taille.\n",
    "    \"\"\"\n",
    "    zip_path = notebook_path.replace(\".ipynb\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(notebook_path)\n",
    "    print(f\"Notebook compress√© et sauvegard√© sous : {zip_path}\")\n",
    "\n",
    "def delete_large_files():\n",
    "    \"\"\"\n",
    "    Supprimer tous les fichiers volumineux tels que .pth, .h5 et les checkpoints.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\") or file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Fichier supprim√© : {file_path}\")\n",
    "\n",
    "\n",
    "    if os.path.exists('.ipynb_checkpoints'):\n",
    "        os.system(\"rm -rf .ipynb_checkpoints\")\n",
    "        print(\"Checkpoints supprim√©s.\")\n",
    "\n",
    "    print(\"Nettoyage des fichiers volumineux termin√© !\")\n",
    "\n",
    "notebook_name = \"CGNet_BSD500.ipynb\"\n",
    "\n",
    "\n",
    "clear_notebook_output(notebook_name, \"notebook_all√©g√©.ipynb\")\n",
    "zip_notebook(\"notebook_all√©g√©.ipynb\")\n",
    "delete_large_files()\n",
    "\n",
    "print(\"\\nüì¶ Op√©rations termin√©es ! Le notebook a √©t√© nettoy√©, compress√© et les fichiers inutiles supprim√©s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54360735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
