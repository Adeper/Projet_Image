{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd32179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08415a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) \n",
    "                            if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def add_gaussian_noise(self, image):\n",
    "        noise_level = np.random.uniform(0.02, 0.1)\n",
    "        noisy_image = image + noise_level * torch.randn_like(image)\n",
    "        return torch.clip(noisy_image, 0., 1.)\n",
    "\n",
    "    def add_salt_pepper_noise(self, image, prob=0.02):\n",
    "        noisy_image = image.clone()\n",
    "        num_pixels = image.numel()\n",
    "        num_pepper = int(prob * num_pixels * 0.5)\n",
    "        pepper_coords = (torch.randint(0, image.shape[1], (num_pepper,)), torch.randint(0, image.shape[2], (num_pepper,)))\n",
    "        noisy_image[:, pepper_coords[0], pepper_coords[1]] = 0\n",
    "        num_salt = int(prob * num_pixels * 0.5)\n",
    "        salt_coords = (torch.randint(0, image.shape[1], (num_salt,)), torch.randint(0, image.shape[2], (num_salt,)))\n",
    "        noisy_image[:, salt_coords[0], salt_coords[1]] = 1\n",
    "        return noisy_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        noisy_image = self.add_gaussian_noise(image)\n",
    "        noisy_image = self.add_salt_pepper_noise(noisy_image)\n",
    "        return noisy_image, image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = DenoisingDataset(root_dir='archive/images/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f721d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextGuidedBlock(nn.Module):\n",
    "    def __init__(self, nIn, nOut, dilation_rate=2):\n",
    "        super(ContextGuidedBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(nIn, nOut // 2, kernel_size=1)\n",
    "        self.conv3x3 = nn.Conv2d(nOut // 2, nOut, kernel_size=3, padding=dilation_rate, dilation=dilation_rate)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class ContextModule(nn.Module):\n",
    "    def __init__(self, nIn, nOut):\n",
    "        super(ContextModule, self).__init__()\n",
    "        self.conv_dil1 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_dil2 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_dil3 = nn.Conv2d(nIn, nOut, kernel_size=3, padding=3, dilation=3)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_dil1(x)\n",
    "        x2 = self.conv_dil2(x)\n",
    "        x3 = self.conv_dil3(x)\n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class CGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CGNet, self).__init__()\n",
    "        self.block1 = ContextGuidedBlock(3, 32)\n",
    "        self.block2 = ContextGuidedBlock(32, 64)\n",
    "        self.res_block1 = self._make_res_block(64, 64)\n",
    "        self.context_module = ContextModule(64, 64)\n",
    "        self.res_block2 = self._make_res_block(64, 64)\n",
    "        self.conv_final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "    \n",
    "    def _make_res_block(self, nIn, nOut):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(nIn, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(nOut, nOut, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(nOut),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1)\n",
    "        x2 = self.res_block1(x2)\n",
    "        x_context = self.context_module(x2)\n",
    "        x3 = self.res_block2(x_context)\n",
    "        output = self.conv_final(x3)\n",
    "        return output\n",
    "\n",
    "model = CGNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d28436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "for file in glob.glob(\"archive/images/train/Thumbs.db\"):\n",
    "    os.remove(file)\n",
    "print(\"Tous les fichiers Thumbs.db ont été supprimés.\")\n",
    "\n",
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    \"\"\"\n",
    "    Calcule le score NIMA après un pré-traitement pour améliorer la qualité perçue.\n",
    "    \"\"\"\n",
    "    enhanced_image = enhance_image(image_tensor)\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(enhanced_image.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        nima_score = torch.sum(scores * torch.arange(1, 11).to(device)).item()\n",
    "    return nima_score\n",
    "def enhance_image(image):\n",
    "    \"\"\"\n",
    "    Pré-traitement pour améliorer la qualité esthétique avant d'utiliser NIMA.\n",
    "    \"\"\"\n",
    "    image = F.adjust_saturation(image, 1.8)\n",
    "    image = F.adjust_contrast(image, 1.5)\n",
    "    image = F.adjust_brightness(image, 1.2)\n",
    "    \n",
    "    sharpen_kernel = torch.tensor(\n",
    "        [[[[0, -1, 0], [-1, 5, -1], [0, -1, 0]]]], \n",
    "        dtype=torch.float32, \n",
    "        device=device\n",
    "    )\n",
    "    channels = []\n",
    "    for c in range(3):\n",
    "        channel = image[c:c+1].unsqueeze(0)\n",
    "        sharpened_channel = torch.nn.functional.conv2d(channel, sharpen_kernel, padding=1)\n",
    "        channels.append(sharpened_channel)\n",
    "    enhanced_image = torch.cat(channels, dim=1)\n",
    "    return torch.clip(enhanced_image, 0., 1.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Entraînement du modèle CGNet avec supervision multi-niveaux\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_imgs, clean_imgs in tqdm(train_loader):\n",
    "        noisy_imgs = noisy_imgs.to(device)\n",
    "        clean_imgs = clean_imgs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, intermediate = model(noisy_imgs)\n",
    "        \n",
    "        loss1 = criterion(outputs, clean_imgs)  \n",
    "        loss2 = criterion(intermediate, clean_imgs)  \n",
    "        loss = loss1 + 0.5 * loss2  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'cgnet_complete.pth')\n",
    "print(\"Modèle sauvegardé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIMAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIMAModel, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights='IMAGENET1K_V2')\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.base_model.last_channel, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "nima_model = NIMAModel().to(device)\n",
    "nima_model.eval()\n",
    "\n",
    "def calculate_nima_score(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        scores = nima_model(image_tensor.unsqueeze(0).to(device))\n",
    "        scores = torch.softmax(scores, dim=1)\n",
    "        return torch.sum(scores * torch.arange(1, 11).to(device)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_with_metrics(model, dataset, index):\n",
    "    \"\"\"\n",
    "    Afficher les images bruitées, originales, et débruitées avec les scores PSNR et NIMA.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    noisy_img, clean_img = dataset[index]\n",
    "\n",
    "    noisy_img = noisy_img.unsqueeze(0).to(device)\n",
    "    clean_img = clean_img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(noisy_img)\n",
    "        output = output.cpu().squeeze()\n",
    "    \n",
    "    psnr_noisy = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), noisy_img.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    psnr_denoised = psnr(clean_img.squeeze().permute(1, 2, 0).cpu().numpy(), output.permute(1, 2, 0).numpy())\n",
    "    \n",
    "    nima_noisy = calculate_nima_score(noisy_img.squeeze())\n",
    "    nima_clean = calculate_nima_score(clean_img.squeeze())\n",
    "    nima_denoised = calculate_nima_score(output)\n",
    "    \n",
    "    print(f\"PSNR Noisy: {psnr_noisy:.2f}, PSNR Denoised: {psnr_denoised:.2f}\")\n",
    "    print(f\"NIMA Noisy: {nima_noisy:.2f}, NIMA Clean: {nima_clean:.2f}, NIMA Denoised: {nima_denoised:.2f}\")\n",
    "    \n",
    "    # Préparer les images pour affichage\n",
    "    noisy_img = noisy_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    clean_img = clean_img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    output = output.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # S'assurer que les valeurs des images sont entre 0 et 1\n",
    "    noisy_img = np.clip(noisy_img, 0, 1)\n",
    "    clean_img = np.clip(clean_img, 0, 1)\n",
    "    output = np.clip(output, 0, 1)\n",
    "    \n",
    "    # Afficher les images\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Noisy Image\\nNIMA: {nima_noisy:.2f}\")\n",
    "    plt.imshow(noisy_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Clean Image\\nNIMA: {nima_clean:.2f}\")\n",
    "    plt.imshow(clean_img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(f\"Denoised Image\\nPSNR: {psnr_denoised:.2f}, NIMA: {nima_denoised:.2f}\")\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Forcer l'installation de nbformat dans l'environnement actuel du notebook\n",
    "!{sys.executable} -m pip install --upgrade nbformat\n",
    "\n",
    "import nbformat\n",
    "import zipfile\n",
    "\n",
    "def clear_notebook_output(notebook_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Nettoyer toutes les sorties d'un notebook Jupyter pour le rendre plus léger.\n",
    "    \"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "\n",
    "    output_path = output_path or notebook_path\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    print(f\"Nettoyage terminé ! Notebook allégé sauvegardé sous : {output_path}\")\n",
    "\n",
    "def zip_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Compresser le notebook nettoyé pour réduire sa taille.\n",
    "    \"\"\"\n",
    "    zip_path = notebook_path.replace(\".ipynb\", \".zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(notebook_path)\n",
    "    print(f\"Notebook compressé et sauvegardé sous : {zip_path}\")\n",
    "\n",
    "def delete_large_files():\n",
    "    \"\"\"\n",
    "    Supprimer tous les fichiers volumineux tels que .pth, .h5 et les checkpoints.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\") or file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Fichier supprimé : {file_path}\")\n",
    "\n",
    "\n",
    "    if os.path.exists('.ipynb_checkpoints'):\n",
    "        os.system(\"rm -rf .ipynb_checkpoints\")\n",
    "        print(\"Checkpoints supprimés.\")\n",
    "\n",
    "    print(\"Nettoyage des fichiers volumineux terminé !\")\n",
    "\n",
    "notebook_name = \"CGNet_BSD500.ipynb\"\n",
    "\n",
    "\n",
    "clear_notebook_output(notebook_name, \"notebook_allégé.ipynb\")\n",
    "zip_notebook(\"notebook_allégé.ipynb\")\n",
    "delete_large_files()\n",
    "\n",
    "print(\"\\n📦 Opérations terminées ! Le notebook a été nettoyé, compressé et les fichiers inutiles supprimés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54360735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "show_sample_with_metrics(model, train_dataset, 0)\n",
    "show_sample_with_metrics(model, train_dataset, 1)\n",
    "show_sample_with_metrics(model, train_dataset, 2)\n",
    "show_sample_with_metrics(model, train_dataset, 3)\n",
    "show_sample_with_metrics(model, train_dataset, 4)\n",
    "show_sample_with_metrics(model, train_dataset, 5)\n",
    "show_sample_with_metrics(model, train_dataset, 6)\n",
    "show_sample_with_metrics(model, train_dataset, 7)\n",
    "show_sample_with_metrics(model, train_dataset, 8)\n",
    "show_sample_with_metrics(model, train_dataset, 9)\n",
    "show_sample_with_metrics(model, train_dataset, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
